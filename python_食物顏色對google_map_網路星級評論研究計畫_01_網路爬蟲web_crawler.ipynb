{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEUIIowuyHGC"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import pyautogui\n",
        "from selenium.webdriver import ActionChains\n",
        "from pynput.keyboard import Key, Controller\n",
        "import pandas as pd\n",
        "import requests\n",
        "from selenium.webdriver.common.by import By\n",
        "from pathlib import Path\n",
        "def find_data():\n",
        "    driver = webdriver.Chrome(\"D:\\東海研究所\\論文\\chromedriver.exe\")\n",
        "    driver.get(\"https://www.google.com.tw/maps/search/%E5%8F%B0%E4%B8%AD%E7%BE%8E%E9%A3%9F/@24.1623009,120.6227675,14z/data=!3m1!4b1?hl=zh-TW\")\n",
        "    for h in range(1, 51):\n",
        "        element = driver.find_element_by_xpath('//*[@id=\"pane\"]/div/div[1]/div/div/div[2]/div[1]/div[3]/div/a')\n",
        "        actionChains = ActionChains(driver)\n",
        "        actionChains.context_click(element).send_keys(Keys.ARROW_DOWN).perform()\n",
        "        time.sleep(3)\n",
        "        keyboard = Controller()\n",
        "        keyboard.press(Key.esc)\n",
        "        keyboard.release(Key.esc)\n",
        "        time.sleep(2)\n",
        "        for j in range(15):\n",
        "                time.sleep(0.5)\n",
        "                pyautogui.press('pgdn')\n",
        "\n",
        "        soup = BeautifulSoup(driver.page_source)\n",
        "        href1 = soup.find_all('a', {'class': 'a4gq8e-aVTXAb-haAclf-jRmmHf-hSRGPd'})\n",
        "        href_link_list = []\n",
        "        title_list = []\n",
        "        for i in href1:\n",
        "                href_link = i.get('href')\n",
        "                title = i.get('aria-label')\n",
        "                href_link_list.append(href_link)\n",
        "                title_list.append(title)\n",
        "        find_star = soup.find_all('span',{'class':'MW4etd'})\n",
        "        star_list = []\n",
        "        for r in find_star:\n",
        "            star_list.append(r.text)\n",
        "        find_comm = soup.find_all('span',{'class':'UY7F9'})\n",
        "        comm_list = []\n",
        "        for kk in find_comm:\n",
        "            comm_list.append(kk.text.replace('(','').replace(')','').replace(',',''))\n",
        "        df = pd.DataFrame()\n",
        "        df['店名'] = title_list\n",
        "        df['網址'] = href_link_list\n",
        "        df['星級'] = star_list\n",
        "        df['評論'] = comm_list\n",
        "        df.to_csv('data/'+str(h)+'商家網址連結.csv', index=False, encoding='utf-8-sig')\n",
        "        driver.find_element_by_xpath('//*[@id=\"ppdPk-Ej1Yeb-LgbsSe-tJiF1e\"]').click()\n",
        "        time.sleep(3)\n",
        "find_data()\n",
        "\n",
        "def marge_data():\n",
        "    excel_dir = Path('D:\\東海研究所\\論文\\data')\n",
        "    excel_files = excel_dir.glob('*.csv')\n",
        "    df = pd.DataFrame()\n",
        "    for xls in excel_files:\n",
        "        data = pd.read_csv(xls, sep=',' )\n",
        "        df = df.append(data)\n",
        "    df.to_csv(\"商家網址連結.csv\", encoding='utf-8-sig', index=False, sep=',')\n",
        "marge_data()\n",
        "\n",
        "\n",
        "def select_data():\n",
        "    df = pd.read_csv('商家網址連結.csv')\n",
        "    df = df[df[\"評論\"] > 500]\n",
        "    df.to_csv('ee.csv', encoding='utf-8-sig', index=False)\n",
        "select_data()\n",
        "\n",
        "\n",
        "def get_image():\n",
        "    df = pd.read_csv('ee.csv')\n",
        "    link = list(df['網址'])\n",
        "    for i in range(len(link)):\n",
        "        try:\n",
        "            driver = webdriver.Chrome(\"D:\\東海研究所\\論文\\chromedriver.exe\")\n",
        "            driver.get(link[i])\n",
        "            time.sleep(3)\n",
        "            driver.find_element_by_xpath(\n",
        "                '//*[@id=\"pane\"]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[1]/h1/span[1]').click()\n",
        "            time.sleep(3)\n",
        "            for j in range(1):\n",
        "                time.sleep(0.5)\n",
        "                pyautogui.press('pgdn')\n",
        "            time.sleep(3)\n",
        "            driver.find_element(By.XPATH, '//button[@aria-label=\"餐飲\"]').click()\n",
        "            driver.find_element_by_xpath('//*[@id=\"pane\"]/div/div[1]/div/div/div[3]/div[1]/div[1]/div/a').click()\n",
        "            time.sleep(2)\n",
        "            for j in range(5):\n",
        "                time.sleep(0.5)\n",
        "                pyautogui.press('pgdn')\n",
        "            time.sleep(3)\n",
        "            soup = BeautifulSoup(driver.page_source)\n",
        "            img = soup.find_all('div', {'class': 'mWq4Rd-HiaYvf-MNynB-gevUs'})\n",
        "            ti = soup.find('div', {'class': 'piCU0 gm2-headline-6'}).text.replace('/', '').replace('|', '')\n",
        "            img_list = []\n",
        "            img_list1 = []\n",
        "            for j in img:\n",
        "                img_list.append(j.get('style').strip('background-image').replace(': url(\"', '').replace('\");', '').replace(':url(//:0)', ''))\n",
        "            for i in img_list:\n",
        "                if len(i)>0:\n",
        "                    img_list1.append(i)\n",
        "            for p in range(len(img_list1)):\n",
        "                img = requests.get(img_list1[p])\n",
        "                with open(\"images\\\\\" + str(ti) + str(p + 1) + \".jpg\", \"wb\") as file:  # 開啟資料夾及命名圖片檔\n",
        "                    file.write(img.content)\n",
        "                file.close()\n",
        "            print(img_list1)\n",
        "            driver.close()\n",
        "        except:\n",
        "            print(\"none\")\n",
        "get_image()"
      ]
    }
  ]
}